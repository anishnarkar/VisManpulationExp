{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",dilation_rate=2, kernel_initializer = 'he_normal')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",dilation_rate=2, kernel_initializer = 'he_normal')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_ritnet(x, filter_size, size, dropout, batch_norm=False):\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_shape, inputs, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True, second=False, conv_1=[]):\n",
    "    '''\n",
    "    UNet, \n",
    "    \n",
    "    '''\n",
    "    # network structure\n",
    "    FILTER_NUM = 64 # number of filters for the first layer\n",
    "    FILTER_SIZE = 3 # size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "    \n",
    "\n",
    "#     inputs = layers.Input(input_shape, dtype=tf.float32)\n",
    "\n",
    "    # Downsampling layers\n",
    "    # DownRes 1, convolution + pooling\n",
    "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
    "    # DownRes 2\n",
    "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
    "    # DownRes 3\n",
    "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
    "    # DownRes 4\n",
    "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
    "    # DownRes 5, convolution only\n",
    "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # Upsampling layers\n",
    "   \n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
    "    if second:\n",
    "        up_16 = layers.concatenate([up_16, conv_16, conv_1[3]], axis=3)\n",
    "    else:\n",
    "        up_16 = layers.concatenate([up_16, conv_16], axis=3)\n",
    "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 7\n",
    "    \n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
    "    if second:\n",
    "        up_32 = layers.concatenate([up_32, conv_32, conv_1[2]], axis=3)\n",
    "    else:\n",
    "        up_32 = layers.concatenate([up_32, conv_32], axis=3)\n",
    "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 8\n",
    "    \n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
    "    if second:\n",
    "        up_64 = layers.concatenate([up_64, conv_64, conv_1[1]], axis=3)\n",
    "    else:\n",
    "        up_64 = layers.concatenate([up_64, conv_64], axis=3)\n",
    "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 9\n",
    "   \n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
    "    if second:\n",
    "        up_128 = layers.concatenate([up_128, conv_128,conv_1[0]], axis=3)\n",
    "    else:\n",
    "        up_128 = layers.concatenate([up_128, conv_128], axis=3)\n",
    "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "   \n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
    "    \n",
    "    \n",
    "    return conv_final,[conv_128, conv_64, conv_32, conv_16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_Unet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "    inputs1 = layers.Input(input_shape, dtype=tf.float32)\n",
    "    output1, pre_conv = UNet(input_shape, inputs1, NUM_CLASSES=1, dropout_rate=0.2, batch_norm=True)\n",
    "    inputs2 = inputs1 * output1\n",
    "    output2,_ =  UNet(input_shape, inputs2, NUM_CLASSES=1, dropout_rate=0.2, \n",
    "                      batch_norm=True, second=True, conv_1=pre_conv)\n",
    "    conv_final = layers.concatenate([output1, output2], axis=3)\n",
    "    # 1*1 convolutional layers\n",
    "   \n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(conv_final)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
    "    \n",
    "    model = models.Model(inputs1, conv_final, name=\"DoubleUNet\")\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128,128,3)\n",
    "model = double_Unet(input_shape, NUM_CLASSES=1, dropout_rate=0.35, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DoubleUNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 128)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 512)  2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 512)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  2359808     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 512)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 1024)   4096        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 1024)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   9438208     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 8, 8, 1024)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 16, 16, 1024) 0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1536) 0           up_sampling2d[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  2359808     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 16, 512)  0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 512)  0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 256)  0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 256)  0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 128)  0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 128 0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_3[0][0]            \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128, 128, 64) 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  65          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 128, 1)  4           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, 128, 1)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 128, 128, 3) 0           input_1[0][0]                    \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 64) 1792        tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 128, 128, 64) 256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128, 128, 64) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 128, 64) 256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 128, 64) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128, 64) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  147584      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 64, 128)  0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 128)  0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 256)  1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 256)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 256)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 512)  2048        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 512)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 512)  2359808     activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 512)  2048        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 512)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 512)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 1024)   4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 1024)   9438208     activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 1024)   4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 8, 8, 1024)   0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 1024) 0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 2048) 0           up_sampling2d_4[0][0]            \n",
      "                                                                 dropout_12[0][0]                 \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 512)  9437696     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 512)  2048        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 512)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 512)  2359808     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 512)  2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 512)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 512)  0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 512)  0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d_5[0][0]            \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 256)  2359552     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 256)  1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 256)  0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 dropout_10[0][0]                 \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 128)  589952      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 128)  147584      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 64, 64, 128)  0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 128 0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 256 0           up_sampling2d_7[0][0]            \n",
      "                                                                 dropout_9[0][0]                  \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, 128, 64) 147520      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 128, 128, 64) 256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 128, 128, 64) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 128, 128, 64) 36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128, 128, 64) 256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 128, 128, 64) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 128, 128, 64) 0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 128, 1)  65          dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 128, 128, 1)  4           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 128, 128, 1)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 2)  0           activation_18[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 128, 1)  3           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128, 128, 1)  4           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, 128, 1)  0           batch_normalization_38[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 65,938,449\n",
      "Trainable params: 65,914,891\n",
      "Non-trainable params: 23,558\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        img = cv2.resize(img, (128,128),interpolation = cv2.INTER_NEAREST)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.python.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= \"C:/Users/anark/Downloads/seg_data/train_images\"\n",
    "images = load_images_from_folder(folder)\n",
    "image_array = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= \"C:/Users/anark/Downloads/seg_data/train_masks\"\n",
    "masks = load_images_from_folder(folder)\n",
    "mask_array = np.array(masks)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 128, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image_array/255\n",
    "\n",
    "img.shape\n",
    "\n",
    "mas = mask_array/255\n",
    "\n",
    "mas[mas > 0.5] = 1\n",
    "mas[mas <= 0.5] = 0\n",
    "mas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500, 128, 128, 3), (2500, 128, 128))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape, mask_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(img, mas, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float16)\n",
    "X_val = X_val.astype(np.float16)\n",
    "y_train = y_train.astype(np.float16)\n",
    "y_val = y_val.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float16'), dtype('float16'), dtype('float16'), dtype('float16'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype, X_val.dtype, y_train.dtype, y_val.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2125, 128, 128, 3), (2125, 128, 128))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float16), array([0., 1.], dtype=float16))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_val),np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_val),np.max(X_train),np.max(y_val),np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DoubleUNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 128)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 512)  2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 512)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  2359808     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 512)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 1024)   4096        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 1024)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   9438208     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 8, 8, 1024)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 16, 16, 1024) 0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1536) 0           up_sampling2d[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  2359808     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 16, 512)  0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 512)  0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 256)  0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 256)  0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 128)  0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 128 0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_3[0][0]            \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128, 128, 64) 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  65          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 128, 1)  4           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, 128, 1)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 128, 128, 3) 0           input_1[0][0]                    \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 64) 1792        tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 128, 128, 64) 256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128, 128, 64) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 128, 64) 256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 128, 64) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128, 64) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  147584      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 64, 128)  0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 128)  0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 256)  1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 256)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 256)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 512)  2048        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 512)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 512)  2359808     activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 512)  2048        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 512)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 512)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 1024)   4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 1024)   9438208     activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 1024)   4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 8, 8, 1024)   0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 1024) 0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 2048) 0           up_sampling2d_4[0][0]            \n",
      "                                                                 dropout_12[0][0]                 \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 512)  9437696     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 512)  2048        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 512)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 512)  2359808     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 512)  2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 512)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 512)  0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 512)  0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d_5[0][0]            \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 256)  2359552     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 256)  1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 256)  0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 dropout_10[0][0]                 \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 128)  589952      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 128)  147584      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 64, 64, 128)  0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 128 0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 256 0           up_sampling2d_7[0][0]            \n",
      "                                                                 dropout_9[0][0]                  \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, 128, 64) 147520      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 128, 128, 64) 256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 128, 128, 64) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 128, 128, 64) 36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128, 128, 64) 256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 128, 128, 64) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 128, 128, 64) 0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 128, 1)  65          dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 128, 128, 1)  4           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 128, 128, 1)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 2)  0           activation_18[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 128, 1)  3           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128, 128, 1)  4           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, 128, 1)  0           batch_normalization_38[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 65,938,449\n",
      "Trainable params: 65,914,891\n",
      "Non-trainable params: 23,558\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=bce_dice_loss,\n",
    "              metrics=[dice_loss])\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'C:/Users/anark/Downloads/seg_data/tmp/weights_DoubleUnet_1.hdf5'\n",
    "log_dir = \"C:/Users/anark/Downloads/seg_data/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_dice_loss',\n",
    "    min_delta=0,\n",
    "    patience=20,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.7159 - dice_loss: 0.3769\n",
      "Epoch 00001: loss improved from inf to 0.71594, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 125s 471ms/step - loss: 0.7159 - dice_loss: 0.3769 - val_loss: 0.6816 - val_dice_loss: 0.3590 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.5571 - dice_loss: 0.3095\n",
      "Epoch 00002: loss improved from 0.71594 to 0.55706, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 123s 463ms/step - loss: 0.5571 - dice_loss: 0.3095 - val_loss: 0.4767 - val_dice_loss: 0.2676 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.4512 - dice_loss: 0.2581\n",
      "Epoch 00003: loss improved from 0.55706 to 0.45117, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 476ms/step - loss: 0.4512 - dice_loss: 0.2581 - val_loss: 0.4012 - val_dice_loss: 0.2295 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.3736 - dice_loss: 0.2170\n",
      "Epoch 00004: loss improved from 0.45117 to 0.37358, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 126s 474ms/step - loss: 0.3736 - dice_loss: 0.2170 - val_loss: 0.4039 - val_dice_loss: 0.2186 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.3110 - dice_loss: 0.1824\n",
      "Epoch 00005: loss improved from 0.37358 to 0.31095, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 478ms/step - loss: 0.3110 - dice_loss: 0.1824 - val_loss: 0.2812 - val_dice_loss: 0.1628 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2646 - dice_loss: 0.1554\n",
      "Epoch 00006: loss improved from 0.31095 to 0.26458, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.2646 - dice_loss: 0.1554 - val_loss: 0.2425 - val_dice_loss: 0.1421 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2282 - dice_loss: 0.1339\n",
      "Epoch 00007: loss improved from 0.26458 to 0.22823, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 128s 480ms/step - loss: 0.2282 - dice_loss: 0.1339 - val_loss: 0.2092 - val_dice_loss: 0.1219 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2011 - dice_loss: 0.1173\n",
      "Epoch 00008: loss improved from 0.22823 to 0.20113, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 479ms/step - loss: 0.2011 - dice_loss: 0.1173 - val_loss: 0.1824 - val_dice_loss: 0.1049 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1818 - dice_loss: 0.1049\n",
      "Epoch 00009: loss improved from 0.20113 to 0.18184, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.1818 - dice_loss: 0.1049 - val_loss: 0.1735 - val_dice_loss: 0.0967 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1633 - dice_loss: 0.0935\n",
      "Epoch 00010: loss improved from 0.18184 to 0.16330, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.1633 - dice_loss: 0.0935 - val_loss: 0.1532 - val_dice_loss: 0.0868 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1480 - dice_loss: 0.0840\n",
      "Epoch 00011: loss improved from 0.16330 to 0.14801, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 128s 482ms/step - loss: 0.1480 - dice_loss: 0.0840 - val_loss: 0.3434 - val_dice_loss: 0.1576 - lr: 9.0484e-04\n",
      "Epoch 12/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1379 - dice_loss: 0.0775\n",
      "Epoch 00012: loss improved from 0.14801 to 0.13785, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 479ms/step - loss: 0.1379 - dice_loss: 0.0775 - val_loss: 0.1315 - val_dice_loss: 0.0727 - lr: 8.1873e-04\n",
      "Epoch 13/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1272 - dice_loss: 0.0711\n",
      "Epoch 00013: loss improved from 0.13785 to 0.12719, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.1272 - dice_loss: 0.0711 - val_loss: 0.1245 - val_dice_loss: 0.0683 - lr: 7.4082e-04\n",
      "Epoch 14/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1200 - dice_loss: 0.0666\n",
      "Epoch 00014: loss improved from 0.12719 to 0.11997, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 128s 481ms/step - loss: 0.1200 - dice_loss: 0.0666 - val_loss: 0.1222 - val_dice_loss: 0.0659 - lr: 6.7032e-04\n",
      "Epoch 15/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1136 - dice_loss: 0.0626\n",
      "Epoch 00015: loss improved from 0.11997 to 0.11357, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 129s 485ms/step - loss: 0.1136 - dice_loss: 0.0626 - val_loss: 0.1167 - val_dice_loss: 0.0627 - lr: 6.0653e-04\n",
      "Epoch 16/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1084 - dice_loss: 0.0593\n",
      "Epoch 00016: loss improved from 0.11357 to 0.10836, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 130s 488ms/step - loss: 0.1084 - dice_loss: 0.0593 - val_loss: 0.1111 - val_dice_loss: 0.0591 - lr: 5.4881e-04\n",
      "Epoch 17/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1038 - dice_loss: 0.0565\n",
      "Epoch 00017: loss improved from 0.10836 to 0.10379, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 131s 493ms/step - loss: 0.1038 - dice_loss: 0.0565 - val_loss: 0.1061 - val_dice_loss: 0.0559 - lr: 4.9659e-04\n",
      "Epoch 18/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0993 - dice_loss: 0.0538\n",
      "Epoch 00018: loss improved from 0.10379 to 0.09929, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 132s 495ms/step - loss: 0.0993 - dice_loss: 0.0538 - val_loss: 0.1067 - val_dice_loss: 0.0555 - lr: 4.4933e-04\n",
      "Epoch 19/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0956 - dice_loss: 0.0516\n",
      "Epoch 00019: loss improved from 0.09929 to 0.09564, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 132s 495ms/step - loss: 0.0956 - dice_loss: 0.0516 - val_loss: 0.1023 - val_dice_loss: 0.0530 - lr: 4.0657e-04\n",
      "Epoch 20/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0921 - dice_loss: 0.0495\n",
      "Epoch 00020: loss improved from 0.09564 to 0.09211, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 133s 500ms/step - loss: 0.0921 - dice_loss: 0.0495 - val_loss: 0.1030 - val_dice_loss: 0.0526 - lr: 3.6788e-04\n",
      "Epoch 21/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0887 - dice_loss: 0.0475\n",
      "Epoch 00021: loss improved from 0.09211 to 0.08866, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 135s 507ms/step - loss: 0.0887 - dice_loss: 0.0475 - val_loss: 0.1002 - val_dice_loss: 0.0508 - lr: 3.3287e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0863 - dice_loss: 0.0460\n",
      "Epoch 00022: loss improved from 0.08866 to 0.08628, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 135s 508ms/step - loss: 0.0863 - dice_loss: 0.0460 - val_loss: 0.0975 - val_dice_loss: 0.0485 - lr: 3.0119e-04\n",
      "Epoch 23/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0844 - dice_loss: 0.0448\n",
      "Epoch 00023: loss improved from 0.08628 to 0.08438, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 132s 497ms/step - loss: 0.0844 - dice_loss: 0.0448 - val_loss: 0.1081 - val_dice_loss: 0.0530 - lr: 2.7253e-04\n",
      "Epoch 24/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0805 - dice_loss: 0.0428\n",
      "Epoch 00024: loss improved from 0.08438 to 0.08047, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 136s 512ms/step - loss: 0.0805 - dice_loss: 0.0428 - val_loss: 0.0983 - val_dice_loss: 0.0482 - lr: 2.4660e-04\n",
      "Epoch 25/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0779 - dice_loss: 0.0414\n",
      "Epoch 00025: loss improved from 0.08047 to 0.07793, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 136s 510ms/step - loss: 0.0779 - dice_loss: 0.0414 - val_loss: 0.0978 - val_dice_loss: 0.0477 - lr: 2.2313e-04\n",
      "Epoch 26/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0755 - dice_loss: 0.0400\n",
      "Epoch 00026: loss improved from 0.07793 to 0.07551, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 136s 512ms/step - loss: 0.0755 - dice_loss: 0.0400 - val_loss: 0.0977 - val_dice_loss: 0.0470 - lr: 2.0190e-04\n",
      "Epoch 27/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0731 - dice_loss: 0.0387\n",
      "Epoch 00027: loss improved from 0.07551 to 0.07308, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 135s 508ms/step - loss: 0.0731 - dice_loss: 0.0387 - val_loss: 0.0908 - val_dice_loss: 0.0439 - lr: 1.8268e-04\n",
      "Epoch 28/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0708 - dice_loss: 0.0375\n",
      "Epoch 00028: loss improved from 0.07308 to 0.07077, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0708 - dice_loss: 0.0375 - val_loss: 0.1018 - val_dice_loss: 0.0474 - lr: 1.6530e-04\n",
      "Epoch 29/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0692 - dice_loss: 0.0366\n",
      "Epoch 00029: loss improved from 0.07077 to 0.06916, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 131s 494ms/step - loss: 0.0692 - dice_loss: 0.0366 - val_loss: 0.1030 - val_dice_loss: 0.0477 - lr: 1.4957e-04\n",
      "Epoch 30/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0666 - dice_loss: 0.0354\n",
      "Epoch 00030: loss improved from 0.06916 to 0.06665, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 502ms/step - loss: 0.0666 - dice_loss: 0.0354 - val_loss: 0.0964 - val_dice_loss: 0.0447 - lr: 1.3534e-04\n",
      "Epoch 31/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0648 - dice_loss: 0.0344\n",
      "Epoch 00031: loss improved from 0.06665 to 0.06482, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0648 - dice_loss: 0.0344 - val_loss: 0.0979 - val_dice_loss: 0.0449 - lr: 1.2246e-04\n",
      "Epoch 32/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0634 - dice_loss: 0.0336\n",
      "Epoch 00032: loss improved from 0.06482 to 0.06338, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0634 - dice_loss: 0.0336 - val_loss: 0.0959 - val_dice_loss: 0.0440 - lr: 1.1080e-04\n",
      "Epoch 33/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0617 - dice_loss: 0.0327\n",
      "Epoch 00033: loss improved from 0.06338 to 0.06168, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 504ms/step - loss: 0.0617 - dice_loss: 0.0327 - val_loss: 0.1006 - val_dice_loss: 0.0451 - lr: 1.0026e-04\n",
      "Epoch 34/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0599 - dice_loss: 0.0318\n",
      "Epoch 00034: loss improved from 0.06168 to 0.05990, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0599 - dice_loss: 0.0318 - val_loss: 0.1015 - val_dice_loss: 0.0449 - lr: 9.0718e-05\n",
      "Epoch 35/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0588 - dice_loss: 0.0312\n",
      "Epoch 00035: loss improved from 0.05990 to 0.05878, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0588 - dice_loss: 0.0312 - val_loss: 0.1003 - val_dice_loss: 0.0442 - lr: 8.2085e-05\n",
      "Epoch 36/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0574 - dice_loss: 0.0305\n",
      "Epoch 00036: loss improved from 0.05878 to 0.05735, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 136s 510ms/step - loss: 0.0574 - dice_loss: 0.0305 - val_loss: 0.0970 - val_dice_loss: 0.0428 - lr: 7.4273e-05\n",
      "Epoch 37/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0562 - dice_loss: 0.0299\n",
      "Epoch 00037: loss improved from 0.05735 to 0.05624, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 136s 513ms/step - loss: 0.0562 - dice_loss: 0.0299 - val_loss: 0.0968 - val_dice_loss: 0.0425 - lr: 6.7205e-05\n",
      "Epoch 38/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0554 - dice_loss: 0.0295\n",
      "Epoch 00038: loss improved from 0.05624 to 0.05542, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 138s 518ms/step - loss: 0.0554 - dice_loss: 0.0295 - val_loss: 0.1023 - val_dice_loss: 0.0443 - lr: 6.0810e-05\n",
      "Epoch 39/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0544 - dice_loss: 0.0290\n",
      "Epoch 00039: loss improved from 0.05542 to 0.05441, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 138s 518ms/step - loss: 0.0544 - dice_loss: 0.0290 - val_loss: 0.0969 - val_dice_loss: 0.0423 - lr: 5.5023e-05\n",
      "Epoch 40/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0534 - dice_loss: 0.0285\n",
      "Epoch 00040: loss improved from 0.05441 to 0.05336, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 138s 520ms/step - loss: 0.0534 - dice_loss: 0.0285 - val_loss: 0.0991 - val_dice_loss: 0.0428 - lr: 4.9787e-05\n",
      "Epoch 41/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0524 - dice_loss: 0.0280\n",
      "Epoch 00041: loss improved from 0.05336 to 0.05244, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 137s 516ms/step - loss: 0.0524 - dice_loss: 0.0280 - val_loss: 0.1003 - val_dice_loss: 0.0430 - lr: 4.5049e-05\n",
      "Epoch 42/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0518 - dice_loss: 0.0277\n",
      "Epoch 00042: loss improved from 0.05244 to 0.05184, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 137s 517ms/step - loss: 0.0518 - dice_loss: 0.0277 - val_loss: 0.0959 - val_dice_loss: 0.0412 - lr: 4.0762e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0512 - dice_loss: 0.0274\n",
      "Epoch 00043: loss improved from 0.05184 to 0.05122, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 139s 521ms/step - loss: 0.0512 - dice_loss: 0.0274 - val_loss: 0.0996 - val_dice_loss: 0.0424 - lr: 3.6883e-05\n",
      "Epoch 44/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0505 - dice_loss: 0.0270\n",
      "Epoch 00044: loss improved from 0.05122 to 0.05048, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 138s 517ms/step - loss: 0.0505 - dice_loss: 0.0270 - val_loss: 0.0968 - val_dice_loss: 0.0412 - lr: 3.3373e-05\n",
      "Epoch 45/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0499 - dice_loss: 0.0267\n",
      "Epoch 00045: loss improved from 0.05048 to 0.04992, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 137s 514ms/step - loss: 0.0499 - dice_loss: 0.0267 - val_loss: 0.0974 - val_dice_loss: 0.0414 - lr: 3.0197e-05\n",
      "Epoch 46/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0496 - dice_loss: 0.0265\n",
      "Epoch 00046: loss improved from 0.04992 to 0.04959, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 504ms/step - loss: 0.0496 - dice_loss: 0.0265 - val_loss: 0.0994 - val_dice_loss: 0.0419 - lr: 2.7324e-05\n",
      "Epoch 47/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0491 - dice_loss: 0.0263\n",
      "Epoch 00047: loss improved from 0.04959 to 0.04907, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0491 - dice_loss: 0.0263 - val_loss: 0.0996 - val_dice_loss: 0.0419 - lr: 2.4723e-05\n",
      "Epoch 48/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0488 - dice_loss: 0.0262\n",
      "Epoch 00048: loss improved from 0.04907 to 0.04883, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 502ms/step - loss: 0.0488 - dice_loss: 0.0262 - val_loss: 0.0987 - val_dice_loss: 0.0414 - lr: 2.2371e-05\n",
      "Epoch 49/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0483 - dice_loss: 0.0259\n",
      "Epoch 00049: loss improved from 0.04883 to 0.04832, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0483 - dice_loss: 0.0259 - val_loss: 0.0963 - val_dice_loss: 0.0406 - lr: 2.0242e-05\n",
      "Epoch 50/50\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0480 - dice_loss: 0.0257\n",
      "Epoch 00050: loss improved from 0.04832 to 0.04798, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5\n",
      "266/266 [==============================] - 134s 503ms/step - loss: 0.0480 - dice_loss: 0.0257 - val_loss: 0.0980 - val_dice_loss: 0.0410 - lr: 1.8316e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2000d2ee490>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4: Fit the u-net model\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_1.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "model.fit(X_train,y_train,epochs=50,batch_size=8,validation_data=(X_val, y_val),callbacks=[tensorboard_callback,model_checkpoint,scheduler_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'C:/Users/anark/Downloads/seg_data/tmp/weights_DoubleUnet_2.hdf5'\n",
    "log_dir = \"C:/Users/anark/Downloads/seg_data/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DoubleUNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 128, 128, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 128, 128, 64) 256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 128, 128, 64) 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 128, 128, 64) 36928       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 128, 128, 64) 256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 128, 128, 64) 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 128, 128, 64) 0           activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 64, 64, 64)   0           dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 64, 64, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 64, 64, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 128)  147584      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 64, 64, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 64, 64, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 64, 64, 128)  0           activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 32, 32, 128)  0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 32, 32, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 32, 32, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 32, 32, 256)  590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 32, 32, 256)  1024        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 32, 32, 256)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 32, 32, 256)  0           activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 16, 16, 256)  0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 512)  2048        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 16, 16, 512)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 512)  2359808     activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 512)  2048        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 16, 16, 512)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16, 16, 512)  0           activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 8, 8, 512)    0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 1024)   4096        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 1024)   9438208     activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 1024)   4096        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 8, 8, 1024)   0           activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling2D) (None, 16, 16, 1024) 0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 1536) 0           up_sampling2d_16[0][0]           \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 512)  2048        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 16, 16, 512)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 512)  2359808     activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 512)  2048        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 16, 16, 512)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 16, 16, 512)  0           activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling2D) (None, 32, 32, 512)  0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_17[0][0]           \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 32, 32, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 256)  590080      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 32, 32, 256)  1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 256)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 32, 32, 256)  0           activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling2D) (None, 64, 64, 256)  0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_18[0][0]           \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 64, 64, 128)  512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 64, 64, 128)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 64, 64, 128)  147584      activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 64, 64, 128)  512         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 64, 64, 128)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 64, 64, 128)  0           activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling2D) (None, 128, 128, 128 0           dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_19[0][0]           \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 128, 128, 64) 256         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 128, 128, 64) 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 128, 128, 64) 36928       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 128, 128, 64) 256         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 128, 128, 64) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 128, 128, 64) 0           activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 128, 128, 1)  65          dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 128, 128, 1)  4           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 128, 128, 1)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_2 (TensorFlowOp [(None, 128, 128, 3) 0           input_3[0][0]                    \n",
      "                                                                 activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 128, 128, 64) 1792        tf_op_layer_Mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 128, 128, 64) 256         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 128, 128, 64) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 128, 128, 64) 36928       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 128, 128, 64) 256         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 128, 128, 64) 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 128, 128, 64) 0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 64, 64, 64)   0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 64, 64, 128)  512         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 64, 64, 128)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 64, 64, 128)  147584      activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 64, 64, 128)  512         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 64, 64, 128)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 64, 64, 128)  0           activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 32, 32, 128)  0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 32, 32, 256)  1024        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 32, 32, 256)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 32, 32, 256)  590080      activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 32, 32, 256)  1024        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 32, 32, 256)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 32, 32, 256)  0           activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 16, 16, 256)  0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359808     activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 512)  2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 512)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 16, 16, 512)  0           activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 8, 8, 512)    0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 8, 8, 1024)   4719616     max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 8, 8, 1024)   4096        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 8, 8, 1024)   9438208     activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 8, 8, 1024)   4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 8, 8, 1024)   0           activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling2D) (None, 16, 16, 1024) 0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 2048) 0           up_sampling2d_20[0][0]           \n",
      "                                                                 dropout_48[0][0]                 \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 16, 16, 512)  9437696     concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 16, 16, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 16, 16, 512)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 16, 16, 512)  2359808     activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 16, 16, 512)  2048        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 16, 16, 512)  0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 16, 16, 512)  0           activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling2D) (None, 32, 32, 512)  0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 32, 32, 1024) 0           up_sampling2d_21[0][0]           \n",
      "                                                                 dropout_47[0][0]                 \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 32, 32, 256)  2359552     concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 32, 32, 256)  1024        conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 32, 32, 256)  0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 32, 32, 256)  590080      activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 32, 32, 256)  1024        conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 32, 32, 256)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 32, 32, 256)  0           activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_22 (UpSampling2D) (None, 64, 64, 256)  0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 64, 64, 512)  0           up_sampling2d_22[0][0]           \n",
      "                                                                 dropout_46[0][0]                 \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 64, 64, 128)  589952      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 64, 64, 128)  512         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 64, 64, 128)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 64, 64, 128)  147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 64, 64, 128)  512         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 128)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 64, 64, 128)  0           activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_23 (UpSampling2D) (None, 128, 128, 128 0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 128, 128, 256 0           up_sampling2d_23[0][0]           \n",
      "                                                                 dropout_45[0][0]                 \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 128, 128, 64) 147520      concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 128, 128, 64) 256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 128, 128, 64) 0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 128, 128, 64) 36928       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 128, 128, 64) 256         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 128, 128, 64) 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 128, 128, 64) 0           activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 128, 128, 1)  65          dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 128, 128, 1)  4           conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 128, 128, 1)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 128, 128, 2)  0           activation_96[0][0]              \n",
      "                                                                 activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 128, 128, 1)  3           concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 128, 128, 1)  4           conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 128, 128, 1)  0           batch_normalization_116[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 65,938,449\n",
      "Trainable params: 65,914,891\n",
      "Non-trainable params: 23,558\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = double_Unet(input_shape, NUM_CLASSES=1, dropout_rate=0.35, batch_norm=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=bce_dice_loss,\n",
    "              metrics=[dice_loss])\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.7184 - dice_loss: 0.3779\n",
      "Epoch 00001: loss improved from inf to 0.71842, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 128s 483ms/step - loss: 0.7184 - dice_loss: 0.3779 - val_loss: 0.6984 - val_dice_loss: 0.3846 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.5580 - dice_loss: 0.3099\n",
      "Epoch 00002: loss improved from 0.71842 to 0.55804, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.5580 - dice_loss: 0.3099 - val_loss: 0.4708 - val_dice_loss: 0.2686 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.4505 - dice_loss: 0.2577\n",
      "Epoch 00003: loss improved from 0.55804 to 0.45050, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 476ms/step - loss: 0.4505 - dice_loss: 0.2577 - val_loss: 0.3773 - val_dice_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.3737 - dice_loss: 0.2169\n",
      "Epoch 00004: loss improved from 0.45050 to 0.37372, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 478ms/step - loss: 0.3737 - dice_loss: 0.2169 - val_loss: 0.3709 - val_dice_loss: 0.2065 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.3118 - dice_loss: 0.1825\n",
      "Epoch 00005: loss improved from 0.37372 to 0.31178, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 128s 482ms/step - loss: 0.3118 - dice_loss: 0.1825 - val_loss: 0.2715 - val_dice_loss: 0.1596 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2638 - dice_loss: 0.1549\n",
      "Epoch 00006: loss improved from 0.31178 to 0.26383, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 487ms/step - loss: 0.2638 - dice_loss: 0.1549 - val_loss: 0.2350 - val_dice_loss: 0.1376 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2287 - dice_loss: 0.1339\n",
      "Epoch 00007: loss improved from 0.26383 to 0.22872, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 488ms/step - loss: 0.2287 - dice_loss: 0.1339 - val_loss: 0.2050 - val_dice_loss: 0.1191 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.2012 - dice_loss: 0.1170\n",
      "Epoch 00008: loss improved from 0.22872 to 0.20122, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 485ms/step - loss: 0.2012 - dice_loss: 0.1170 - val_loss: 0.1797 - val_dice_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1784 - dice_loss: 0.1031\n",
      "Epoch 00009: loss improved from 0.20122 to 0.17842, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 490ms/step - loss: 0.1784 - dice_loss: 0.1031 - val_loss: 0.1636 - val_dice_loss: 0.0931 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1614 - dice_loss: 0.0923\n",
      "Epoch 00010: loss improved from 0.17842 to 0.16136, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 131s 492ms/step - loss: 0.1614 - dice_loss: 0.0923 - val_loss: 0.1477 - val_dice_loss: 0.0832 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1471 - dice_loss: 0.0834\n",
      "Epoch 00011: loss improved from 0.16136 to 0.14714, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 131s 491ms/step - loss: 0.1471 - dice_loss: 0.0834 - val_loss: 0.1405 - val_dice_loss: 0.0780 - lr: 9.0484e-04\n",
      "Epoch 12/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1369 - dice_loss: 0.0768\n",
      "Epoch 00012: loss improved from 0.14714 to 0.13686, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 488ms/step - loss: 0.1369 - dice_loss: 0.0768 - val_loss: 0.1328 - val_dice_loss: 0.0725 - lr: 8.1873e-04\n",
      "Epoch 13/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1288 - dice_loss: 0.0716\n",
      "Epoch 00013: loss improved from 0.13686 to 0.12878, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 485ms/step - loss: 0.1288 - dice_loss: 0.0716 - val_loss: 0.1276 - val_dice_loss: 0.0693 - lr: 7.4082e-04\n",
      "Epoch 14/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1207 - dice_loss: 0.0667\n",
      "Epoch 00014: loss improved from 0.12878 to 0.12075, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 483ms/step - loss: 0.1207 - dice_loss: 0.0667 - val_loss: 0.1196 - val_dice_loss: 0.0646 - lr: 6.7032e-04\n",
      "Epoch 15/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1136 - dice_loss: 0.0625\n",
      "Epoch 00015: loss improved from 0.12075 to 0.11361, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 485ms/step - loss: 0.1136 - dice_loss: 0.0625 - val_loss: 0.1150 - val_dice_loss: 0.0609 - lr: 6.0653e-04\n",
      "Epoch 16/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1080 - dice_loss: 0.0590\n",
      "Epoch 00016: loss improved from 0.11361 to 0.10800, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.1080 - dice_loss: 0.0590 - val_loss: 0.1108 - val_dice_loss: 0.0587 - lr: 5.4881e-04\n",
      "Epoch 17/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.1042 - dice_loss: 0.0565\n",
      "Epoch 00017: loss improved from 0.10800 to 0.10417, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 128s 480ms/step - loss: 0.1042 - dice_loss: 0.0565 - val_loss: 0.1085 - val_dice_loss: 0.0570 - lr: 4.9659e-04\n",
      "Epoch 18/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0999 - dice_loss: 0.0539\n",
      "Epoch 00018: loss improved from 0.10417 to 0.09990, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 479ms/step - loss: 0.0999 - dice_loss: 0.0539 - val_loss: 0.1049 - val_dice_loss: 0.0548 - lr: 4.4933e-04\n",
      "Epoch 19/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0958 - dice_loss: 0.0516\n",
      "Epoch 00019: loss improved from 0.09990 to 0.09585, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.0958 - dice_loss: 0.0516 - val_loss: 0.1016 - val_dice_loss: 0.0524 - lr: 4.0657e-04\n",
      "Epoch 20/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0922 - dice_loss: 0.0494\n",
      "Epoch 00020: loss improved from 0.09585 to 0.09221, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 476ms/step - loss: 0.0922 - dice_loss: 0.0494 - val_loss: 0.1005 - val_dice_loss: 0.0513 - lr: 3.6788e-04\n",
      "Epoch 21/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0884 - dice_loss: 0.0472\n",
      "Epoch 00021: loss improved from 0.09221 to 0.08841, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.0884 - dice_loss: 0.0472 - val_loss: 0.0994 - val_dice_loss: 0.0500 - lr: 3.3287e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0861 - dice_loss: 0.0458\n",
      "Epoch 00022: loss improved from 0.08841 to 0.08613, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 477ms/step - loss: 0.0861 - dice_loss: 0.0458 - val_loss: 0.1004 - val_dice_loss: 0.0500 - lr: 3.0119e-04\n",
      "Epoch 23/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0837 - dice_loss: 0.0444\n",
      "Epoch 00023: loss improved from 0.08613 to 0.08373, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 487ms/step - loss: 0.0837 - dice_loss: 0.0444 - val_loss: 0.0944 - val_dice_loss: 0.0472 - lr: 2.7253e-04\n",
      "Epoch 24/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0803 - dice_loss: 0.0426\n",
      "Epoch 00024: loss improved from 0.08373 to 0.08032, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 488ms/step - loss: 0.0803 - dice_loss: 0.0426 - val_loss: 0.0949 - val_dice_loss: 0.0468 - lr: 2.4660e-04\n",
      "Epoch 25/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0776 - dice_loss: 0.0411\n",
      "Epoch 00025: loss improved from 0.08032 to 0.07757, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 487ms/step - loss: 0.0776 - dice_loss: 0.0411 - val_loss: 0.0917 - val_dice_loss: 0.0451 - lr: 2.2313e-04\n",
      "Epoch 26/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0759 - dice_loss: 0.0401\n",
      "Epoch 00026: loss improved from 0.07757 to 0.07592, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 485ms/step - loss: 0.0759 - dice_loss: 0.0401 - val_loss: 0.0932 - val_dice_loss: 0.0452 - lr: 2.0190e-04\n",
      "Epoch 27/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0731 - dice_loss: 0.0386\n",
      "Epoch 00027: loss improved from 0.07592 to 0.07308, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 484ms/step - loss: 0.0731 - dice_loss: 0.0386 - val_loss: 0.0900 - val_dice_loss: 0.0436 - lr: 1.8268e-04\n",
      "Epoch 28/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0708 - dice_loss: 0.0374\n",
      "Epoch 00028: loss improved from 0.07308 to 0.07076, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 128s 480ms/step - loss: 0.0708 - dice_loss: 0.0374 - val_loss: 0.0883 - val_dice_loss: 0.0423 - lr: 1.6530e-04\n",
      "Epoch 29/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0685 - dice_loss: 0.0362\n",
      "Epoch 00029: loss improved from 0.07076 to 0.06854, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 479ms/step - loss: 0.0685 - dice_loss: 0.0362 - val_loss: 0.0911 - val_dice_loss: 0.0428 - lr: 1.4957e-04\n",
      "Epoch 30/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0666 - dice_loss: 0.0352\n",
      "Epoch 00030: loss improved from 0.06854 to 0.06657, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 139s 522ms/step - loss: 0.0666 - dice_loss: 0.0352 - val_loss: 0.0896 - val_dice_loss: 0.0420 - lr: 1.3534e-04\n",
      "Epoch 31/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0645 - dice_loss: 0.0341\n",
      "Epoch 00031: loss improved from 0.06657 to 0.06451, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 127s 478ms/step - loss: 0.0645 - dice_loss: 0.0341 - val_loss: 0.0876 - val_dice_loss: 0.0405 - lr: 1.2246e-04\n",
      "Epoch 32/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0631 - dice_loss: 0.0334\n",
      "Epoch 00032: loss improved from 0.06451 to 0.06308, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 131s 492ms/step - loss: 0.0631 - dice_loss: 0.0334 - val_loss: 0.0887 - val_dice_loss: 0.0410 - lr: 1.1080e-04\n",
      "Epoch 33/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0619 - dice_loss: 0.0328\n",
      "Epoch 00033: loss improved from 0.06308 to 0.06195, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 131s 494ms/step - loss: 0.0619 - dice_loss: 0.0328 - val_loss: 0.0883 - val_dice_loss: 0.0405 - lr: 1.0026e-04\n",
      "Epoch 34/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0601 - dice_loss: 0.0318\n",
      "Epoch 00034: loss improved from 0.06195 to 0.06006, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 132s 496ms/step - loss: 0.0601 - dice_loss: 0.0318 - val_loss: 0.0887 - val_dice_loss: 0.0401 - lr: 9.0718e-05\n",
      "Epoch 35/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0585 - dice_loss: 0.0311\n",
      "Epoch 00035: loss improved from 0.06006 to 0.05854, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 131s 492ms/step - loss: 0.0585 - dice_loss: 0.0311 - val_loss: 0.0880 - val_dice_loss: 0.0397 - lr: 8.2085e-05\n",
      "Epoch 36/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0573 - dice_loss: 0.0305\n",
      "Epoch 00036: loss improved from 0.05854 to 0.05734, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 130s 490ms/step - loss: 0.0573 - dice_loss: 0.0305 - val_loss: 0.0873 - val_dice_loss: 0.0393 - lr: 7.4273e-05\n",
      "Epoch 37/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0563 - dice_loss: 0.0299\n",
      "Epoch 00037: loss improved from 0.05734 to 0.05630, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 129s 484ms/step - loss: 0.0563 - dice_loss: 0.0299 - val_loss: 0.0880 - val_dice_loss: 0.0392 - lr: 6.7205e-05\n",
      "Epoch 38/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0554 - dice_loss: 0.0294\n",
      "Epoch 00038: loss improved from 0.05630 to 0.05536, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 456ms/step - loss: 0.0554 - dice_loss: 0.0294 - val_loss: 0.0876 - val_dice_loss: 0.0388 - lr: 6.0810e-05\n",
      "Epoch 39/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0543 - dice_loss: 0.0289\n",
      "Epoch 00039: loss improved from 0.05536 to 0.05428, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0543 - dice_loss: 0.0289 - val_loss: 0.0880 - val_dice_loss: 0.0388 - lr: 5.5023e-05\n",
      "Epoch 40/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0533 - dice_loss: 0.0284\n",
      "Epoch 00040: loss improved from 0.05428 to 0.05333, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0533 - dice_loss: 0.0284 - val_loss: 0.0869 - val_dice_loss: 0.0382 - lr: 4.9787e-05\n",
      "Epoch 41/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0527 - dice_loss: 0.0281\n",
      "Epoch 00041: loss improved from 0.05333 to 0.05266, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0527 - dice_loss: 0.0281 - val_loss: 0.0877 - val_dice_loss: 0.0383 - lr: 4.5049e-05\n",
      "Epoch 42/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0518 - dice_loss: 0.0277\n",
      "Epoch 00042: loss improved from 0.05266 to 0.05184, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0518 - dice_loss: 0.0277 - val_loss: 0.0877 - val_dice_loss: 0.0382 - lr: 4.0762e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0513 - dice_loss: 0.0274\n",
      "Epoch 00043: loss improved from 0.05184 to 0.05131, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0513 - dice_loss: 0.0274 - val_loss: 0.0879 - val_dice_loss: 0.0381 - lr: 3.6883e-05\n",
      "Epoch 44/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0505 - dice_loss: 0.0270\n",
      "Epoch 00044: loss improved from 0.05131 to 0.05051, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0505 - dice_loss: 0.0270 - val_loss: 0.0874 - val_dice_loss: 0.0378 - lr: 3.3373e-05\n",
      "Epoch 45/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0502 - dice_loss: 0.0268\n",
      "Epoch 00045: loss improved from 0.05051 to 0.05021, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0502 - dice_loss: 0.0268 - val_loss: 0.0885 - val_dice_loss: 0.0381 - lr: 3.0197e-05\n",
      "Epoch 46/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0497 - dice_loss: 0.0266\n",
      "Epoch 00046: loss improved from 0.05021 to 0.04972, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0497 - dice_loss: 0.0266 - val_loss: 0.0876 - val_dice_loss: 0.0377 - lr: 2.7324e-05\n",
      "Epoch 47/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0493 - dice_loss: 0.0264\n",
      "Epoch 00047: loss improved from 0.04972 to 0.04934, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0493 - dice_loss: 0.0264 - val_loss: 0.0877 - val_dice_loss: 0.0376 - lr: 2.4723e-05\n",
      "Epoch 48/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0490 - dice_loss: 0.0262\n",
      "Epoch 00048: loss improved from 0.04934 to 0.04898, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0490 - dice_loss: 0.0262 - val_loss: 0.0873 - val_dice_loss: 0.0374 - lr: 2.2371e-05\n",
      "Epoch 49/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0485 - dice_loss: 0.0260\n",
      "Epoch 00049: loss improved from 0.04898 to 0.04855, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0485 - dice_loss: 0.0260 - val_loss: 0.0881 - val_dice_loss: 0.0375 - lr: 2.0242e-05\n",
      "Epoch 50/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0482 - dice_loss: 0.0259\n",
      "Epoch 00050: loss improved from 0.04855 to 0.04821, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0482 - dice_loss: 0.0259 - val_loss: 0.0879 - val_dice_loss: 0.0374 - lr: 1.8316e-05\n",
      "Epoch 51/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0480 - dice_loss: 0.0258\n",
      "Epoch 00051: loss improved from 0.04821 to 0.04804, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0480 - dice_loss: 0.0258 - val_loss: 0.0873 - val_dice_loss: 0.0372 - lr: 1.6573e-05\n",
      "Epoch 52/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0478 - dice_loss: 0.0256\n",
      "Epoch 00052: loss improved from 0.04804 to 0.04785, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0478 - dice_loss: 0.0256 - val_loss: 0.0876 - val_dice_loss: 0.0372 - lr: 1.4996e-05\n",
      "Epoch 53/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0474 - dice_loss: 0.0255\n",
      "Epoch 00053: loss improved from 0.04785 to 0.04745, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0474 - dice_loss: 0.0255 - val_loss: 0.0881 - val_dice_loss: 0.0373 - lr: 1.3569e-05\n",
      "Epoch 54/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0473 - dice_loss: 0.0254\n",
      "Epoch 00054: loss improved from 0.04745 to 0.04726, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0473 - dice_loss: 0.0254 - val_loss: 0.0873 - val_dice_loss: 0.0370 - lr: 1.2277e-05\n",
      "Epoch 55/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0470 - dice_loss: 0.0253\n",
      "Epoch 00055: loss improved from 0.04726 to 0.04704, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0470 - dice_loss: 0.0253 - val_loss: 0.0873 - val_dice_loss: 0.0370 - lr: 1.1109e-05\n",
      "Epoch 56/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0468 - dice_loss: 0.0251\n",
      "Epoch 00056: loss improved from 0.04704 to 0.04679, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0468 - dice_loss: 0.0251 - val_loss: 0.0874 - val_dice_loss: 0.0370 - lr: 1.0052e-05\n",
      "Epoch 57/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0466 - dice_loss: 0.0250\n",
      "Epoch 00057: loss improved from 0.04679 to 0.04656, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0466 - dice_loss: 0.0250 - val_loss: 0.0874 - val_dice_loss: 0.0369 - lr: 9.0953e-06\n",
      "Epoch 58/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0465 - dice_loss: 0.0250\n",
      "Epoch 00058: loss improved from 0.04656 to 0.04653, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0465 - dice_loss: 0.0250 - val_loss: 0.0875 - val_dice_loss: 0.0370 - lr: 8.2297e-06\n",
      "Epoch 59/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0465 - dice_loss: 0.0250\n",
      "Epoch 00059: loss improved from 0.04653 to 0.04647, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0465 - dice_loss: 0.0250 - val_loss: 0.0873 - val_dice_loss: 0.0369 - lr: 7.4466e-06\n",
      "Epoch 60/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0462 - dice_loss: 0.0248\n",
      "Epoch 00060: loss improved from 0.04647 to 0.04624, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0462 - dice_loss: 0.0248 - val_loss: 0.0875 - val_dice_loss: 0.0369 - lr: 6.7379e-06\n",
      "Epoch 61/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0462 - dice_loss: 0.0248\n",
      "Epoch 00061: loss improved from 0.04624 to 0.04616, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0462 - dice_loss: 0.0248 - val_loss: 0.0871 - val_dice_loss: 0.0367 - lr: 6.0967e-06\n",
      "Epoch 62/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0461 - dice_loss: 0.0248\n",
      "Epoch 00062: loss improved from 0.04616 to 0.04607, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0461 - dice_loss: 0.0248 - val_loss: 0.0874 - val_dice_loss: 0.0368 - lr: 5.5165e-06\n",
      "Epoch 63/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0459 - dice_loss: 0.0247\n",
      "Epoch 00063: loss improved from 0.04607 to 0.04591, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0459 - dice_loss: 0.0247 - val_loss: 0.0876 - val_dice_loss: 0.0368 - lr: 4.9916e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0459 - dice_loss: 0.0247\n",
      "Epoch 00064: loss improved from 0.04591 to 0.04589, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0459 - dice_loss: 0.0247 - val_loss: 0.0875 - val_dice_loss: 0.0367 - lr: 4.5166e-06\n",
      "Epoch 65/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0458 - dice_loss: 0.0246\n",
      "Epoch 00065: loss improved from 0.04589 to 0.04576, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0458 - dice_loss: 0.0246 - val_loss: 0.0874 - val_dice_loss: 0.0367 - lr: 4.0868e-06\n",
      "Epoch 66/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0457 - dice_loss: 0.0246\n",
      "Epoch 00066: loss improved from 0.04576 to 0.04569, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0457 - dice_loss: 0.0246 - val_loss: 0.0875 - val_dice_loss: 0.0368 - lr: 3.6979e-06\n",
      "Epoch 67/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0455 - dice_loss: 0.0245\n",
      "Epoch 00067: loss improved from 0.04569 to 0.04552, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0455 - dice_loss: 0.0245 - val_loss: 0.0875 - val_dice_loss: 0.0367 - lr: 3.3460e-06\n",
      "Epoch 68/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0456 - dice_loss: 0.0246\n",
      "Epoch 00068: loss did not improve from 0.04552\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0456 - dice_loss: 0.0246 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 3.0275e-06\n",
      "Epoch 69/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0455 - dice_loss: 0.0245\n",
      "Epoch 00069: loss did not improve from 0.04552\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0455 - dice_loss: 0.0245 - val_loss: 0.0874 - val_dice_loss: 0.0367 - lr: 2.7394e-06\n",
      "Epoch 70/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0456 - dice_loss: 0.0245\n",
      "Epoch 00070: loss did not improve from 0.04552\n",
      "266/266 [==============================] - 119s 446ms/step - loss: 0.0456 - dice_loss: 0.0245 - val_loss: 0.0875 - val_dice_loss: 0.0367 - lr: 2.4787e-06\n",
      "Epoch 71/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0456 - dice_loss: 0.0245\n",
      "Epoch 00071: loss did not improve from 0.04552\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0456 - dice_loss: 0.0245 - val_loss: 0.0875 - val_dice_loss: 0.0367 - lr: 2.2429e-06\n",
      "Epoch 72/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0454 - dice_loss: 0.0244\n",
      "Epoch 00072: loss improved from 0.04552 to 0.04542, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 456ms/step - loss: 0.0454 - dice_loss: 0.0244 - val_loss: 0.0874 - val_dice_loss: 0.0367 - lr: 2.0294e-06\n",
      "Epoch 73/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0454 - dice_loss: 0.0245\n",
      "Epoch 00073: loss improved from 0.04542 to 0.04541, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0454 - dice_loss: 0.0245 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.8363e-06\n",
      "Epoch 74/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00074: loss improved from 0.04541 to 0.04531, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0875 - val_dice_loss: 0.0368 - lr: 1.6616e-06\n",
      "Epoch 75/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00075: loss improved from 0.04531 to 0.04528, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.5034e-06\n",
      "Epoch 76/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00076: loss improved from 0.04528 to 0.04526, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.3604e-06\n",
      "Epoch 77/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00077: loss did not improve from 0.04526\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0875 - val_dice_loss: 0.0367 - lr: 1.2309e-06\n",
      "Epoch 78/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0452 - dice_loss: 0.0243\n",
      "Epoch 00078: loss improved from 0.04526 to 0.04516, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0452 - dice_loss: 0.0243 - val_loss: 0.0877 - val_dice_loss: 0.0367 - lr: 1.1138e-06\n",
      "Epoch 79/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00079: loss did not improve from 0.04516\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.0078e-06\n",
      "Epoch 80/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0452 - dice_loss: 0.0243\n",
      "Epoch 00080: loss did not improve from 0.04516\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0452 - dice_loss: 0.0243 - val_loss: 0.0875 - val_dice_loss: 0.0366 - lr: 9.1188e-07\n",
      "Epoch 81/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00081: loss improved from 0.04516 to 0.04504, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 8.2510e-07\n",
      "Epoch 82/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0452 - dice_loss: 0.0244\n",
      "Epoch 00082: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0452 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 7.4658e-07\n",
      "Epoch 83/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00083: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 6.7554e-07\n",
      "Epoch 84/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00084: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 6.1125e-07\n",
      "Epoch 85/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00085: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 5.5308e-07\n",
      "Epoch 86/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0453 - dice_loss: 0.0244\n",
      "Epoch 00086: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0453 - dice_loss: 0.0244 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 5.0045e-07\n",
      "Epoch 87/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00087: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 4.5283e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00088: loss did not improve from 0.04504\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 4.0973e-07\n",
      "Epoch 89/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0242\n",
      "Epoch 00089: loss improved from 0.04504 to 0.04495, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0450 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 3.7074e-07\n",
      "Epoch 90/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00090: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 119s 447ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 3.3546e-07\n",
      "Epoch 91/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00091: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 119s 448ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 3.0354e-07\n",
      "Epoch 92/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0452 - dice_loss: 0.0243\n",
      "Epoch 00092: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 119s 449ms/step - loss: 0.0452 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 2.7465e-07\n",
      "Epoch 93/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00093: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 450ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 2.4852e-07\n",
      "Epoch 94/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00094: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 2.2487e-07\n",
      "Epoch 95/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00095: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 2.0347e-07\n",
      "Epoch 96/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00096: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.8410e-07\n",
      "Epoch 97/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00097: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 1.6659e-07\n",
      "Epoch 98/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00098: loss did not improve from 0.04495\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.5073e-07\n",
      "Epoch 99/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0449 - dice_loss: 0.0242\n",
      "Epoch 00099: loss improved from 0.04495 to 0.04491, saving model to C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5\n",
      "266/266 [==============================] - 121s 456ms/step - loss: 0.0449 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 1.3639e-07\n",
      "Epoch 100/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00100: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 1.2341e-07\n",
      "Epoch 101/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00101: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 1.1167e-07\n",
      "Epoch 102/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0242\n",
      "Epoch 00102: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0450 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 1.0104e-07\n",
      "Epoch 103/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00103: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 9.1424e-08\n",
      "Epoch 104/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00104: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 453ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0877 - val_dice_loss: 0.0367 - lr: 8.2724e-08\n",
      "Epoch 105/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00105: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 7.4851e-08\n",
      "Epoch 106/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00106: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 6.7728e-08\n",
      "Epoch 107/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00107: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 453ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 6.1283e-08\n",
      "Epoch 108/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00108: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 5.5451e-08\n",
      "Epoch 109/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0449 - dice_loss: 0.0242\n",
      "Epoch 00109: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0449 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 5.0174e-08\n",
      "Epoch 110/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0449 - dice_loss: 0.0242\n",
      "Epoch 00110: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0449 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 4.5400e-08\n",
      "Epoch 111/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00111: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 455ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 4.1079e-08\n",
      "Epoch 112/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0452 - dice_loss: 0.0243\n",
      "Epoch 00112: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0452 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 3.7170e-08\n",
      "Epoch 113/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00113: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 3.3633e-08\n",
      "Epoch 114/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0242\n",
      "Epoch 00114: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 121s 454ms/step - loss: 0.0450 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 3.0432e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00115: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0367 - lr: 2.7536e-08\n",
      "Epoch 116/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0242\n",
      "Epoch 00116: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0450 - dice_loss: 0.0242 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 2.4916e-08\n",
      "Epoch 117/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00117: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 2.2545e-08\n",
      "Epoch 118/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0450 - dice_loss: 0.0243\n",
      "Epoch 00118: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 451ms/step - loss: 0.0450 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 2.0399e-08\n",
      "Epoch 119/200\n",
      "266/266 [==============================] - ETA: 0s - loss: 0.0451 - dice_loss: 0.0243\n",
      "Epoch 00119: loss did not improve from 0.04491\n",
      "266/266 [==============================] - 120s 452ms/step - loss: 0.0451 - dice_loss: 0.0243 - val_loss: 0.0876 - val_dice_loss: 0.0366 - lr: 1.8458e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f196325df0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4: Fit the u-net model\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('C:/Users/anark/Downloads/seg_data/models/unet_STARE_AttentionUnet_2.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "model.fit(X_train,y_train,epochs=200,batch_size=8,validation_data=(X_val, y_val),callbacks=[tensorboard_callback,model_checkpoint,scheduler_callback,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Completed at : 20220830-050451'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Completed at : \" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= \"C:/Users/anark/Downloads/seg_data/test_images\"\n",
    "images = load_images_from_folder(folder)\n",
    "image_array = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= \"C:/Users/anark/Downloads/seg_data/test_masks\"\n",
    "masks = load_images_from_folder(folder)\n",
    "masks_array = np.array(masks)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 128, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image_array/255\n",
    "\n",
    "img.shape\n",
    "\n",
    "mas = masks_array/255\n",
    "\n",
    "mas[mas > 0.5] = 1\n",
    "mas[mas <= 0.5] = 0\n",
    "mas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.astype(np.float16)\n",
    "mas = mas.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:\\Users\\anark\\Downloads\\seg_data\\models\\unet_STARE_AttentionUnet_1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135, 128, 128, 3), (135, 128, 128), (135, 128, 128, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape,mas.shape,pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2940f483b20>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mas[0].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2940fa84a00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 2s/step - loss: 0.0971 - dice_loss: 0.0409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09712888300418854, 0.040942348539829254]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(img,mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:\\Users\\anark\\Downloads\\seg_data\\models\\unet_STARE_AttentionUnet_2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29467bb8250>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mas[35].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2945aac76a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 2s/step - loss: 0.0881 - dice_loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08806893974542618, 0.0373283252120018]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(img,mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
